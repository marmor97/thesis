import torch


import torch
import torch.cuda as cuda

from transformers import AutoTokenizer, AutoModelForCausalLM, MptConfig, GenerationConfig
import transformers
import random


def load_model_and_tokenizer(config):
    '''
    Load the model and tokenizer from the configuration file
    '''

    if cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"
    device = "cpu"
    print("[INFO]: Device is: ", device)
    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'
    device = "cpu"
        
    random.seed(config['experiment']['seed'])
    print("[INFO]: Loading model and tokenizer...")

    model_path = config[config['experiment']['type']]['model_id']
    configuration = transformers.AutoConfig.from_pretrained(model_path, trust_remote_code=True)
    dtype = torch.bfloat16

    configuration.max_seq_len = config[config['experiment']['type']]['context_window'] # (input + output) tokens can now be up to 4096
    configuration.attn_config['alibi'] = config[config['experiment']['type']]['alibi']
    configuration.attn_config['attn_impl'] = config[config['experiment']['type']]['attn_impl']
    configuration.use_cache = config[config['experiment']['type']]['use_cache']
    if config[config['experiment']['type']] == 'em':
        configuration.attn_config['memory_type'] = config[config['experiment']['type']]['memory_type']

    generator = AutoModelForCausalLM.from_pretrained(model_path, 
                                                    config = configuration,
                                                    trust_remote_code=True)
    
    if config['experiment']['type'] == 'em':
        print("[INFO]: Emptying memories...")
        generator.empty_memories() 

    tokenizer = AutoTokenizer.from_pretrained(config[config['experiment']['type']]['tokenizer_id'], padding_side='left')
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    generation_method = config[config['experiment']['type']]['generation']

    if generation_method != "default":
        # Get the parameters for the chosen generation method
        generation_params = config[config['experiment']['type']][generation_method]

        # Get the current configuration
        current_config = generator.generation_config

        # Update the current configuration with the new parameters
        for key, value in generation_params.items():
            current_config.key=value


    return generator, tokenizer


def manual_outputs(input_ids, model, tokenizer, max_length=20):

    decoder_input_ids = [model.config.decoder_start_token_id]
    predicted_ids = []
    for i in range(max_length): 
        outputs = model(input_ids=input_ids, decoder_input_ids=torch.tensor([decoder_input_ids]))
        logits = outputs.logits[:,i,:]
        # perform argmax on the last dimension (i.e. greedy decoding)
        predicted_id = logits.argmax(-1)
        predicted_ids.append(predicted_id.item())
        print(tokenizer.decode([predicted_id.squeeze()]))
        # add predicted id to decoder_input_ids
        decoder_input_ids = decoder_input_ids + [predicted_id]

    return predicted_ids

def collect_logits(output, input_ids):
    logits = []
    # Getting only the output scores for the generated tokens
    # logits_predicted = output.scores[len(input_ids[0]):]
    # print(logits_predicted)
    # print(len(logits_predicted))
    # print(len(output.scores))
    logits_predicted = output.scores
    
    tokens_predicted = output.sequences[0][len(input_ids[0]):]
    print(tokens_predicted)
    print(len(tokens_predicted))

    for i, t in zip(range(len(logits_predicted)), tokens_predicted):
        # Printing the logits for the tokens that were generated by the model
        logits.append(logits_predicted[i][0][t].item())
    
    return logits


# Before this prepare the tokens so they contain 1) docs, 2) question 3) answer
# Calculate the tokens of the answer separately so we can take the answer_lenght and filter out everyhing before if we want to
def perplexity(model, encodings, answer_len, max_length=2048, stride=1, modeltype="em", topk=1):
    
    print("Entered perplexity function...")
    
    nlls = []
    
    # Encode the question and answer
    seq_len = encodings.size(1)
    original_len = encodings.size(1) - answer_len
    begin_loc = 0

    print(original_len)
    for end_loc in range(original_len, seq_len, stride):
        
        #trg_len = end_loc - prev_end_loc  # may be different from stride on last loop
        input_ids = encodings[:, begin_loc:end_loc].to('cpu')
        target_ids = input_ids.clone()
    
        target_ids[:, -1:] = -100

        with torch.no_grad():
            if modeltype=="em":
                outputs = model(input_ids, labels=target_ids, topk=topk)
            else:
                outputs = model(input_ids, labels=target_ids)

            # loss is calculated using CrossEntropyLoss which averages over valid labels
            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels
            # to the left by 1.
            neg_log_likelihood = outputs.loss

        nlls.append(neg_log_likelihood)
        
        if end_loc == seq_len:
            break

    ppl = torch.exp(torch.stack(nlls).mean())
    print("PPL: ", ppl)
    return ppl